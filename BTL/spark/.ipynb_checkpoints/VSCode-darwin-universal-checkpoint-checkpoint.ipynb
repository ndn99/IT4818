{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26160934-323d-447f-9969-e1fef9d5298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_spark = {\n",
    "    \"spark.jars.packages\": \"org.apache.hadoop:hadoop-aws:3.3.2\",\n",
    "    \"spark.hadoop.fs.s3a.access.key\": 'wZPhSZYqzYwCwZBG7H9F',\n",
    "    \"spark.hadoop.fs.s3a.secret.key\": 'laK81FcRGl6Kd6JXi1EYA5V7HGzG25ReFMbpwfKN',\n",
    "    \"spark.hadoop.fs.s3a.endpoint\": 'http://192.168.0.103:9000',\n",
    "    \"spark.hadoop.fs.s3a.path.style.access\": \"true\",\n",
    "    \"spark.hadoop.fs.s3a.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\",\n",
    "    \"spark.hadoop.fs.s3a.connection.ssl.enabled\": \"false\"\n",
    "}\n",
    "\n",
    "\n",
    "def create_spark_session():\n",
    "    spark = SparkSession.builder.master(\"spark://spark:7077\").appName(\"IT4818\")\n",
    "    for key, value in configs_spark.items():\n",
    "        spark.config(key, value)\n",
    "    return spark.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa804b90-8298-4f9c-a2a7-0a61c43224cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b13beb92-228e-4969-84e2-002e2a8d0c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fc95d28-6aff-4d7b-84b4-8353706d3af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = ['Sales_December_2019.csv','Sales_April_2019.csv','Sales_February_2019.csv','Sales_March_2019.csv','Sales_August_2019.csv','Sales_May_2019.csv',\n",
    "             'Sales_November_2019.csv','Sales_October_2019.csv','Sales_January_2019.csv','Sales_September_2019.csv','Sales_July_2019.csv','Sales_June_2019.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3f4f629-536e-439c-abf2-62aef5ac8c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [os.path.join(\"s3a://btl\", f\"{x}\") for x in file_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a1576e-6839-4a0f-b1f3-63b16a7397ea",
   "metadata": {},
   "source": [
    "## 1. What is the total number of sales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d6bb687-7b1e-4070-800b-4738d340b7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3879dd0-6274-4ae6-a2ad-290d65e02631",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o52.csv.\n: java.nio.file.AccessDeniedException: s3a://btl/Sales_April_2019.csv: getFileStatus on s3a://btl/Sales_April_2019.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 17703D6946B3F453; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8:403 Forbidden\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:255)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3796)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$exists$34(S3AFileSystem.java:4703)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4701)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:784)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:782)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 17703D6946B3F453; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5437)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5384)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1367)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$10(S3AFileSystem.java:2545)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2533)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2513)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3776)\n\t... 23 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m jan \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms3a://btl/Sales_April_2019.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:535\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o52.csv.\n: java.nio.file.AccessDeniedException: s3a://btl/Sales_April_2019.csv: getFileStatus on s3a://btl/Sales_April_2019.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 17703D6946B3F453; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8:403 Forbidden\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:255)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3796)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$exists$34(S3AFileSystem.java:4703)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4701)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:784)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:782)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 17703D6946B3F453; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5437)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5384)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1367)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$10(S3AFileSystem.java:2545)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2533)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2513)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3776)\n\t... 23 more\n"
     ]
    }
   ],
   "source": [
    "jan = spark.read.csv('s3a://btl/Sales_April_2019.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8276844f-5114-42b2-91f6-d26d748e0770",
   "metadata": {},
   "outputs": [],
   "source": [
    "jan.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d1e463-1b37-4f4c-a18f-af5708256832",
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_total_sale = jan.withColumn(\"sale\", col(\"Quantity Ordered\") * col(\"Price Each\")).agg(sum(\"sale\").alias(\"total_sale\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bdb493-9373-4ae6-b5fe-ded5b64389b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_total_sale.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77200a36-d695-4dcc-8579-09cd5006b7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([], jan.schema)\n",
    "for csv in csv_files:\n",
    "    df = df.unionAll(spark.read.csv(csv, header=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf689e56-ab7b-4a6a-b7cf-b74274fc4de9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d33768-3a8a-46bf-ba7d-8a3b616541cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d67d319-69ef-49bf-9ea8-fa242978af76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.drop(subset=['Quantity Ordered', 'Price Each'])\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc4ea4f-cad9-47ce-a47c-e064768d6ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sales = df.withColumn(\"sale\", col(\"Quantity Ordered\") * col(\"Price Each\")).agg(sum(\"sale\").alias(\"total_sale\"))\n",
    "total_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80ca87c-b484-4a46-9c5d-07b01fe170e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For visualize\n",
    "labels = []\n",
    "values = []\n",
    "\n",
    "# Jan\n",
    "labels.append(\"Jan\")\n",
    "jan = spark.read.csv('s3a://btl/Sales_January_2019.csv', header=True)\n",
    "values.append(jan.withColumn(\"sale\", col(\"Quantity Ordered\") * col(\"Price Each\")).agg(sum(\"sale\").alias(\"total_sale\")).collect()[0]['total_sale'])\n",
    "\n",
    "# Feb\n",
    "labels.append(\"Feb\")\n",
    "feb = spark.read.csv('s3a://btl/Sales_February_2019.csv', header=True)\n",
    "values.append(feb.withColumn(\"sale\", col(\"Quantity Ordered\") * col(\"Price Each\")).agg(sum(\"sale\").alias(\"total_sale\")).collect()[0]['total_sale'])\n",
    "\n",
    "# Mar\n",
    "labels.append(\"Mar\")\n",
    "mar = spark.read.csv('s3a://btl/Sales_March_2019.csv', header=True)\n",
    "values.append(mar.withColumn(\"sale\", col(\"Quantity Ordered\") * col(\"Price Each\")).agg(sum(\"sale\").alias(\"total_sale\")).collect()[0]['total_sale'])\n",
    "\n",
    "# Apr\n",
    "labels.append(\"Apr\")\n",
    "apr = spark.read.csv('s3a://btl/Sales_April_2019.csv', header=True)\n",
    "values.append(apr.withColumn(\"sale\", col(\"Quantity Ordered\") * col(\"Price Each\")).agg(sum(\"sale\").alias(\"total_sale\")).collect()[0]['total_sale'])\n",
    "\n",
    "# May\n",
    "labels.append(\"May\")\n",
    "may = spark.read.csv('s3a://btl/Sales_May_2019.csv', header=True)\n",
    "values.append(may.withColumn(\"sale\", col(\"Quantity Ordered\") * col(\"Price Each\")).agg(sum(\"sale\").alias(\"total_sale\")).collect()[0]['total_sale'])\n",
    "\n",
    "# Jun\n",
    "labels.append(\"Jun\")\n",
    "jun = spark.read.csv('s3a://btl/Sales_June_2019.csv', header=True)\n",
    "values.append(jun.withColumn(\"sale\", col(\"Quantity Ordered\") * col(\"Price Each\")).agg(sum(\"sale\").alias(\"total_sale\")).collect()[0]['total_sale'])\n",
    "\n",
    "# Jul\n",
    "labels.append(\"Jul\")\n",
    "jul = spark.read.csv('s3a://btl/Sales_July_2019.csv', header=True)\n",
    "values.append(jul.withColumn(\"sale\", col(\"Quantity Ordered\") * col(\"Price Each\")).agg(sum(\"sale\").alias(\"total_sale\")).collect()[0]['total_sale'])\n",
    "\n",
    "# Aug\n",
    "labels.append(\"Aug\")\n",
    "aug = spark.read.csv('s3a://btl/Sales_August_2019.csv', header=True)\n",
    "values.append(aug.withColumn(\"sale\", col(\"Quantity Ordered\") * col(\"Price Each\")).agg(sum(\"sale\").alias(\"total_sale\")).collect()[0]['total_sale'])\n",
    "\n",
    "# Sep\n",
    "labels.append(\"Sep\")\n",
    "sep = spark.read.csv('s3a://btl/Sales_September_2019.csv', header=True)\n",
    "values.append(sep.withColumn(\"sale\", col(\"Quantity Ordered\") * col(\"Price Each\")).agg(sum(\"sale\").alias(\"total_sale\")).collect()[0]['total_sale'])\n",
    "\n",
    "# Oct\n",
    "labels.append(\"Oct\")\n",
    "octo = spark.read.csv('s3a://btl/Sales_October_2019.csv', header=True)\n",
    "values.append(octo.withColumn(\"sale\", col(\"Quantity Ordered\") * col(\"Price Each\")).agg(sum(\"sale\").alias(\"total_sale\")).collect()[0]['total_sale'])\n",
    "\n",
    "# Nov\n",
    "labels.append(\"Nov\")\n",
    "nov = spark.read.csv('s3a://btl/Sales_November_2019.csv', header=True)\n",
    "values.append(nov.withColumn(\"sale\", col(\"Quantity Ordered\") * col(\"Price Each\")).agg(sum(\"sale\").alias(\"total_sale\")).collect()[0]['total_sale'])\n",
    "\n",
    "# Dec\n",
    "labels.append(\"Dec\")\n",
    "dec = spark.read.csv('s3a://btl/Sales_December_2019.csv', header=True)\n",
    "values.append(dec.withColumn(\"sale\", col(\"Quantity Ordered\") * col(\"Price Each\")).agg(sum(\"sale\").alias(\"total_sale\")).collect()[0]['total_sale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7399bf-89ed-45b3-8ef2-5d39c88c05fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1969f92d-b2ee-49e6-a695-9e487b9575a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c02d1b-492d-4db3-8afa-ea12f3eba025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save .csv for elk\n",
    "data = list(zip(labels, values))\n",
    "qs1_df = spark.createDataFrame(data, [\"Month\", \"Total Sale\"])\n",
    "qs1_df.write.csv(\"s3a://btl/ressult/qs1.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7996348c-d983-44a8-838c-5140f557eb91",
   "metadata": {},
   "source": [
    "• What is the average sales per month?\n",
    "• What is the monthly revenue?\n",
    "• What are the key demographics of the customers?\n",
    "• Which market (country) generated the most sales on average?\n",
    "• What were the profits by segment?\n",
    "• When were the best- and worst-selling periods?\n",
    "• Which products sell best?\n",
    "• Which products should the company order more or less of?\n",
    "• How should the company adjust its marketing strategies to VIP customers and less-engaged\n",
    "ones?\n",
    "• Should the company acquire new customers, and how much money should they spend on\n",
    "it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1aef5e-6ca6-428f-9133-39368f8d855d",
   "metadata": {},
   "source": [
    "## 2. What is the average sales per month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2cf119-acd3-4272-a2e1-66e5f4a88e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_sales = total_sales.withColumn('average_sale', col('total_sale') / 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5f1941-2af7-42ee-a697-1de3da5d0d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afc848c-c4c8-4ca3-b6cb-6b38ed8ffd0e",
   "metadata": {},
   "source": [
    "## 3. What is the monthly revenue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283188f8-9b00-4351-8592-3ab5c2328937",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs1_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a309d547-599d-4984-8eab-05ad4aaf3b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0b53e1-1f54-4f83-8d62-554062497364",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10, 5))\n",
    " \n",
    "# creating the bar plot\n",
    "plt.bar(labels, values,\n",
    "        width = 0.4)\n",
    " \n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Total Sale\")\n",
    "plt.title(\"Total Sale in each Month\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bdd9b3-517c-43c8-b784-ba6957436a2e",
   "metadata": {},
   "source": [
    "## 4. What are the key demographics of the customers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f55da7-8d48-47d0-a2e1-46c2687d594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9430c07-63a5-4bd1-a819-d7775f9fec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('area', F.split(col('Purchase Address'), ',')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab39ab-1df6-4cc6-b6a4-9f76c202d007",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889afab3-db81-4c47-aa8c-fbd89e35bfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_count = df.groupBy('area').count()\n",
    "area_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4972b633-a2cf-4153-88ca-9fb51b37acfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_labels = area_count.select(\"area\").rdd.flatMap(lambda x: x).collect()\n",
    "area_values = area_count.select(\"count\").rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f537687-fdf4-4759-9692-50c3e961f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_labels = [\"Unknown\" if item is None else item for item in area_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7507e60-12d0-4808-8e29-fcf078816451",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10, 5))\n",
    " \n",
    "# creating the bar plot\n",
    "plt.bar(area_labels, area_values,\n",
    "        width = 0.4)\n",
    " \n",
    "plt.xlabel(\"Area\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Amount Of Products\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f7ba07-5877-44d7-919e-cbe1321211cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed46bd6-ba9f-4d2c-a0d8-b80afe291393",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_df = df.groupBy('area').agg((F.count('*') / F.lit(df.count()) * 100).alias('percentage'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b255b0-dc12-4894-9c47-6529b791a4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_df.withColumn(\n",
    "    'formatted_percentage',\n",
    "    F.concat(F.format_number(F.col('percentage'), 2), F.lit('%'))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b9dd2f-ad2a-4397-9fdb-c6cc445ea592",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_seller_df = df.groupBy('area', 'Product').agg(F.max('Quantity Ordered').alias('max_quantity'))\n",
    "best_seller_per_area_df = best_seller_df.groupBy('area').agg(\n",
    "    F.first('Product').alias('best_seller_product'),\n",
    "    F.max('max_quantity').alias('highest_quantity')\n",
    ")\n",
    "\n",
    "best_seller_per_area_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d627fcc1-1718-46fa-ad1f-95cd92ebeb28",
   "metadata": {},
   "source": [
    "## 5. Which market (country) generated the most sales on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087128bf-8906-44e5-ad03-9e3c1c6bfcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_total_sale_df = df.withColumn('total', col('Quantity Ordered') * col('Price Each')) \\\n",
    "                  .groupBy('area') \\\n",
    "                  .agg(sum('total').alias('area_total_sale'))\n",
    "area_total_sale_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2878ffa-855a-4a81-b73c-5e7ec8544f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_total_sale_labels = area_total_sale_df.select(\"area\").rdd.flatMap(lambda x: x).collect()\n",
    "area_total_sale_labels.remove(None)\n",
    "area_total_sale_values = area_total_sale_df.select(\"area_total_sale\").rdd.flatMap(lambda x: x).collect()\n",
    "area_total_sale_values.remove(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bccfde0-1b2c-4401-a3b5-80a6e6b74907",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10, 5))\n",
    " \n",
    "# creating the bar plot\n",
    "plt.bar(area_total_sale_labels, area_total_sale_values,\n",
    "        width = 0.4)\n",
    " \n",
    "plt.xlabel(\"Area\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Total Sales\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a20907b-1b9c-44f3-9bb8-628d5a6272e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef4ea41-b7e9-41a6-9ef7-96ae1641a189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
